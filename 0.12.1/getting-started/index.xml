<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Apache Iceberg</title><link>https://samredai.github.io/iceberg-docs-prototype/0.12.1/getting-started/</link><description>Recent content on Apache Iceberg</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://samredai.github.io/iceberg-docs-prototype/0.12.1/getting-started/index.xml" rel="self" type="application/rss+xml"/><item><title>Configuration</title><link>https://samredai.github.io/iceberg-docs-prototype/0.12.1/spark-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://samredai.github.io/iceberg-docs-prototype/0.12.1/spark-configuration/</guid><description>Spark Configuration # Catalogs # Spark 3.0 adds an API to plug in table catalogs that are used to load, create, and manage Iceberg tables. Spark catalogs are configured by setting Spark properties under spark.sql.catalog.
This creates an Iceberg catalog named hive_prod that loads tables from a Hive metastore:
spark.sql.catalog.hive_prod = org.apache.iceberg.spark.SparkCatalog spark.sql.catalog.hive_prod.type = hive spark.sql.catalog.hive_prod.uri = thrift://metastore-host:port # omit uri to use the same URI as Spark: hive.</description></item><item><title>DDL</title><link>https://samredai.github.io/iceberg-docs-prototype/0.12.1/spark-ddl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://samredai.github.io/iceberg-docs-prototype/0.12.1/spark-ddl/</guid><description>Spark DDL # To use Iceberg in Spark, first configure Spark catalogs.
Iceberg uses Apache Spark&amp;rsquo;s DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API with different levels of support in Spark versions. Spark 2.4 does not support SQL DDL.
!!! Note Spark 2.4 can&amp;rsquo;t create Iceberg tables with DDL, instead use Spark 3.x or the Iceberg API.
CREATE TABLE # Spark 3.0 can create tables in any Iceberg catalog with the clause USING iceberg:</description></item><item><title>Procedures</title><link>https://samredai.github.io/iceberg-docs-prototype/0.12.1/spark-procedures/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://samredai.github.io/iceberg-docs-prototype/0.12.1/spark-procedures/</guid><description>Spark Procedures # To use Iceberg in Spark, first configure Spark catalogs. Stored procedures are only available when using Iceberg SQL extensions in Spark 3.x.
Usage # Procedures can be used from any configured Iceberg catalog with CALL. All procedures are in the namespace system.
CALL supports passing arguments by name (recommended) or by position. Mixing position and named arguments is not supported.
Named arguments # All procedure arguments are named.</description></item><item><title>Queries</title><link>https://samredai.github.io/iceberg-docs-prototype/0.12.1/spark-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://samredai.github.io/iceberg-docs-prototype/0.12.1/spark-queries/</guid><description>Spark Queries # To use Iceberg in Spark, first configure Spark catalogs.
Iceberg uses Apache Spark&amp;rsquo;s DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API with different levels of support in Spark versions:
Feature support Spark 3.0 Spark 2.4 Notes SELECT ✔️ DataFrame reads ✔️ ✔️ Metadata table SELECT ✔️ History metadata table ✔️ ✔️ Snapshots metadata table ✔️ ✔️ Files metadata table ✔️ ✔️ Manifests metadata table ✔️ ✔️ Querying with SQL # In Spark 3, tables use identifiers that include a catalog name.</description></item><item><title>Structured Streaming</title><link>https://samredai.github.io/iceberg-docs-prototype/0.12.1/spark-structured-streaming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://samredai.github.io/iceberg-docs-prototype/0.12.1/spark-structured-streaming/</guid><description>Spark Structured Streaming # Iceberg uses Apache Spark&amp;rsquo;s DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API with different levels of support in Spark versions.
As of Spark 3.0, DataFrame reads and writes are supported.
Feature support Spark 3.0 Spark 2.4 Notes DataFrame write ✔ ✔ Streaming Writes # To write values from streaming query to Iceberg table, use DataStreamWriter:</description></item><item><title>Writes</title><link>https://samredai.github.io/iceberg-docs-prototype/0.12.1/spark-writes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://samredai.github.io/iceberg-docs-prototype/0.12.1/spark-writes/</guid><description>Spark Writes # To use Iceberg in Spark, first configure Spark catalogs.
Some plans are only available when using Iceberg SQL extensions in Spark 3.x.
Iceberg uses Apache Spark&amp;rsquo;s DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API with different levels of support in Spark versions:
Feature support Spark 3.0 Spark 2.4 Notes SQL insert into ✔️ SQL merge into ✔️ ⚠ Requires Iceberg Spark extensions SQL insert overwrite ✔️ SQL delete from ✔️ ⚠ Row-level delete requires Spark extensions SQL update ✔️ ⚠ Requires Iceberg Spark extensions DataFrame append ✔️ ✔️ DataFrame overwrite ✔️ ✔️ ⚠ Behavior changed in Spark 3.</description></item></channel></rss>